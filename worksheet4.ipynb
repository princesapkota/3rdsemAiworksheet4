{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cNEr-tQ1aa5Ub_u-0Bh_5JvIiNtT2NNL",
      "authorship_tag": "ABX9TyPyGJSWAcaiDP8sHJXj+md0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/princesapkota/3rdsemAiworksheet4/blob/main/worksheet4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bdCuXuFY5o2l"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 1 start\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/datasets/diabetes.csv\")"
      ],
      "metadata": {
        "id": "lsyRDC-TL5d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa7f82d2-414c-4e2a-adba-26ca97c6c5f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "#getting the first 5 rows of dataframe"
      ],
      "metadata": {
        "id": "k0IND5zMMtiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "#shwoing the information of dataframe"
      ],
      "metadata": {
        "id": "nFkmgeGoMwEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n",
        "#showing detailed columns and row data of dataframe"
      ],
      "metadata": {
        "id": "Vyz7DeZ6MyDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()\n",
        "#showing a whole list of null valued datas"
      ],
      "metadata": {
        "id": "jPhmWZL2M0jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#handling missing values\n",
        "# Checking for zero or anomalous values in specific columns\n",
        "columns_to_check = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "for col in columns_to_check:\n",
        "    print(f\"\\n{col} - Zero value count: {sum(df[col] == 0)}\")\n",
        "#Replacing zero values with the column median\n",
        "for col in columns_to_check:\n",
        "    df[col] = df[col].replace(0, df[col].median())\n",
        "\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "u87mp-7KM2Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#featuring engineering\n",
        "\n",
        "x =df.drop(columns=['Outcome']).values\n",
        "y = df['Outcome'].values\n",
        "#spl;ittting into traim 70% ro 30%\n",
        "np.random.seed(42)\n",
        "indices = np.arange(len(x))\n",
        "np.random.shuffle(indices)\n",
        "#defining the splits\n",
        "splitIndex = int(len(x)*0.7)\n",
        "xTrain, xTest = x[indices[:splitIndex]], x[indices[splitIndex:]]\n",
        "yTrain, yTest = y[indices[:splitIndex]], y[indices[splitIndex:]]\n",
        "print(len(x))\n",
        "print(len(y))\n",
        "print(xTrain[:5])\n",
        "print(yTrain[:5])"
      ],
      "metadata": {
        "id": "eoB2OdjrM4Z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the Euclidean distance between two points\n",
        "def euclidean_distance(point1, point2):\n",
        "    return np.sqrt(np.sum((point1 - point2) ** 2))\n"
      ],
      "metadata": {
        "id": "iUHDUOVUM6z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the class for a single query point\n",
        "def predict_single(xTrain, yTrain, query, k=5):\n",
        "    distances = []\n",
        "\n",
        "    # Calculate distance from the query to all training points\n",
        "    for i in range(len(xTrain)):\n",
        "        distance = euclidean_distance(xTrain[i], query)\n",
        "        distances.append((distance, yTrain[i]))  # Append (distance, label)\n",
        "\n",
        "    # Sort the distances\n",
        "    distances.sort(key=lambda x: x[0])  # Sort by distance\n",
        "\n",
        "    # Get the labels of the k nearest neighbors\n",
        "    k_nearest_labels = [label for _, label in distances[:k]]\n",
        "\n",
        "    # Majority vote: find the most common class\n",
        "    prediction = max(set(k_nearest_labels), key=k_nearest_labels.count)\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "IWcgStDnM9h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict for all test samples\n",
        "def predict(xTrain, yTrain, X_test, k=5):\n",
        "    predictions = []\n",
        "    for query in X_test:\n",
        "        pred = predict_single(xTrain, yTrain, query, k)\n",
        "        predictions.append(pred)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "edipGIQkM_vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to calculate accuracy\n",
        "def accuracy_score(yTrue, yPred):\n",
        "    correct = np.sum(yTrue == yPred)  # Count matching values\n",
        "    return correct / len(yTrue)\n",
        "k = 5\n",
        "yPred = predict(xTrain, yTrain, xTest, k)\n",
        "accuracy = accuracy_score(yTest, yPred)\n",
        "print(f\"Accuracy of KNN model with k={k}: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nFirst 10 predictions vs actual labels:\")\n",
        "print(\"Predicted:\",yPred[:10])\n",
        "print(\"Actual:\", yTest[:10])\n"
      ],
      "metadata": {
        "id": "1dtiP_pXNBzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for minimun to maxu=imum scaling\n",
        "def min_max_scale(X):\n",
        "    X_min = np.min(X, axis=0)\n",
        "    X_max = np.max(X, axis=0)\n",
        "    return (X - X_min) / (X_max - X_min)\n",
        "\n",
        "xTrainScaled = min_max_scale(xTrain)\n",
        "xTestScaled = min_max_scale(xTest)\n",
        "print(xTrainScaled[:5])\n"
      ],
      "metadata": {
        "id": "3HtPzkeCND8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##to training aand testing knn scaled data\n",
        "kNeighbors = 5\n",
        "yPredScaled = predict(xTrainScaled, yTrain, xTestScaled, kNeighbors)\n",
        "accuracyScaled = accuracy_score(yTest, yPredScaled)\n",
        "\n",
        "print(f\"Accuracy with scaled data (k={kNeighbors}): {accuracyScaled * 100:.2f}%\")\n",
        "#comparing\n",
        "print(f\"Accuracy with original data: {accuracy * 100:.2f}%\")\n",
        "print(f\"Accuracy with scaled data: {accuracyScaled * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "xNwEAe4oNF-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PRIBELM 3\n",
        "#predicting a single query wuith costt\n",
        "def predict_single_with_cost(xTrain, yTrain, query, k):\n",
        "    distances = []\n",
        "    cost = 0  # Track computation cost (distance calculations)\n",
        "\n",
        "    for i in range(len(xTrain)):\n",
        "        distance = euclidean_distance(xTrain[i], query)\n",
        "        distances.append((distance, yTrain[i]))\n",
        "        cost += 1\n",
        "\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "    k_nearest_labels = [label for _, label in distances[:k]]\n",
        "    prediction = max(set(k_nearest_labels), key=k_nearest_labels.count)\n",
        "\n",
        "    return prediction, cost\n"
      ],
      "metadata": {
        "id": "EtOCj6RANIOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediciting a wholee query with cost\n",
        "def predict_with_cost(xTrain, yTrain, xTest, k):\n",
        "    predictions = []\n",
        "    total_cost = 0\n",
        "\n",
        "    for query in xTest:\n",
        "        pred, cost = predict_single_with_cost(xTrain, yTrain, query, k)\n",
        "        predictions.append(pred)\n",
        "        total_cost += cost\n",
        "\n",
        "    return np.array(predictions), total_cost\n"
      ],
      "metadata": {
        "id": "y_FGUlSgNKMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#experimenting with k = 1 to 15 values\n",
        "k_values = range(1, 16)\n",
        "accuracy_original = []\n",
        "accuracy_scaled = []\n",
        "cost_original = []\n",
        "cost_scaled = []\n",
        "\n",
        "for k in k_values:\n",
        "    #original dataset\n",
        "    yPredOriginal, costOrig = predict_with_cost(xTrain, yTrain, xTest, k)\n",
        "    accuracy_original.append(accuracy_score(yTest, yPredOriginal))\n",
        "    cost_original.append(costOrig)\n",
        "\n",
        "    #scaled dataset\n",
        "    yPredScaled, costScale = predict_with_cost(xTrainScaled, yTrain, xTestScaled, k)\n",
        "    accuracy_scaled.append(accuracy_score(yTest, yPredScaled))\n",
        "    cost_scaled.append(costScale)\n",
        "\n",
        "print(\"Accuracies with original data:\", accuracy_original)\n",
        "print(\"Accuracies with scaled data:\", accuracy_scaled)\n",
        "print(\"Computation cost with original data:\", cost_original)\n",
        "print(\"Computation cost with scaled data:\", cost_scaled)\n"
      ],
      "metadata": {
        "id": "bDedA8EkNMay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the values results in graph\n",
        "#k vs accuray\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(k_values, accuracy_original, label='Original Dataset')\n",
        "plt.plot(k_values, accuracy_scaled, label='Scaled Dataset')\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('k vs. Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "abYCZW9_NQDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot k vs computational cost gprah\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(k_values, cost_original, label='Original Data Cost')\n",
        "plt.plot(k_values, cost_scaled, label='Scaled Data Cost')\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('Distance Calculations')\n",
        "plt.title('Computation Cost for Different Numbers of Neighbors')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OJKpYmW7NS-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 2 discussion\n",
        "### If the accuracy improves significantly after scaling, it means the original dataset had features with very different ranges, and those differences were messing up the distance calculations. Features with larger values were unfairly dominating the results, making KNN less accurate."
      ],
      "metadata": {
        "id": "H0W6VifeNoz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN works by measuring distances between points to decide which neighbors are closest. If some features have bigger ranges (like income vs. age), those features will overpower the others in the distance calculations. This can lead to poor neighbor selection and lower accuracy.\n",
        "\n",
        "### Scaling fixes this by making all features equally important, ensuring KNN focuses on the actual relationships in the data rather than being biased by size differences."
      ],
      "metadata": {
        "id": "9NOmir4NNyGr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4AlomYoTN4Y5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}